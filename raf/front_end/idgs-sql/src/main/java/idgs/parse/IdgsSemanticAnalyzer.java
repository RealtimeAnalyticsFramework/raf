/*
Copyright (c) <2013>, Intel Corporation All Rights Reserved.

The source code, information and material ("Material") contained herein is owned by Intel Corporation or its suppliers or licensors, and title to such Material remains with Intel Corporation or its suppliers or licensors. The Material contains proprietary information of Intel or its suppliers and licensors. The Material is protected by worldwide copyright laws and treaty provisions. No part of the Material may be used, copied, reproduced, modified, published, uploaded, posted, transmitted, distributed or disclosed in any way without Intel's prior express written permission. No license under any patent, copyright or other intellectual property rights in the Material is granted to or conferred upon you, either expressly, by implication, inducement, estoppel or otherwise. Any license under such intellectual property rights must be express and approved by Intel in writing.

Unless otherwise agreed by Intel in writing, you may not remove or alter this notice or any other notice embedded in Materials by Intel or Intelâ€™s suppliers or licensors in any way.
*/
package idgs.parse;

import idgs.exception.IdgsParseException;
import idgs.execution.IdgsOperator;
import idgs.execution.IdgsTask;
import idgs.execution.IdgsWork;

import java.lang.reflect.Field;
import java.lang.reflect.Method;
import java.util.ArrayList;
import java.util.List;

import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hadoop.hive.metastore.api.FieldSchema;
import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
import org.apache.hadoop.hive.ql.exec.Operator;
import org.apache.hadoop.hive.ql.exec.TaskFactory;
import org.apache.hadoop.hive.ql.optimizer.Optimizer;
import org.apache.hadoop.hive.ql.parse.ASTNode;
import org.apache.hadoop.hive.ql.parse.HiveParser;
import org.apache.hadoop.hive.ql.parse.ParseContext;
import org.apache.hadoop.hive.ql.parse.QB;
import org.apache.hadoop.hive.ql.parse.RowResolver;
import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
import org.apache.hadoop.hive.ql.parse.SemanticException;
import org.apache.hadoop.hive.ql.plan.HiveOperation;
import org.apache.hadoop.hive.ql.plan.OperatorDesc;
import org.apache.hadoop.hive.ql.session.SessionState;

public class IdgsSemanticAnalyzer extends SemanticAnalyzer {

  public IdgsSemanticAnalyzer(HiveConf conf) throws SemanticException {
    super(conf);
  }

  @SuppressWarnings({"unchecked" })
  @Override
  public void analyzeInternal(ASTNode ast) throws SemanticException {
    reset();

    boolean shouldReset = false;

    QB qb = new QB(null, null, false);
    ParseContext pctx = getParseContext();
    pctx.setQB(qb);
    pctx.setParseTree(ast);
    initParseCtx(pctx);
    ASTNode child = ast;

    LOG.info("Starting Semantic Analysis");

    // analyze create table command
    int astTokenType = ast.getToken().getType();
    if (astTokenType == HiveParser.TOK_CREATETABLE) {
      super.analyzeInternal(ast);
      shouldReset = true;
      return;
    } else {
      SessionState.get().setCommandType(HiveOperation.QUERY);
    }

    if (astTokenType == HiveParser.TOK_CREATEVIEW || astTokenType == HiveParser.TOK_ANALYZE) {
      super.analyzeInternal(ast);
      return;
    }

    // Continue analyzing from the child ASTNode.
    if (!doPhase1(child, qb, initPhase1Ctx())) {
      return;
    }

    try {
      // Used to protect against recursive views in getMetaData().
      Field viewsExpandedField = SemanticAnalyzer.class.getDeclaredField("viewsExpanded");
      viewsExpandedField.setAccessible(true);
      viewsExpandedField.set(this, new ArrayList<String>());

      LOG.info("Completed phase 1 of Shark Semantic Analysis");
      getMetaData(qb);
      LOG.info("Completed getting MetaData in Shark Semantic Analysis");

      // Reset makes sure we don't run the mapred jobs generated by Hive.
      if (shouldReset) {
        reset();
      }

      // Save the result schema derived from the sink operator produced
      // by genPlan. This has the correct column names, which clients
      // such as JDBC would prefer instead of the c0, c1 we'll end
      // up with later.
      FileSinkOperator sinkOp = (FileSinkOperator) genPlan(qb);
      List<Operator<? extends OperatorDesc>> op = null;
      if (qb.isSimpleSelectQuery()) {
        op = sinkOp.getParentOperators();
      }

      // Use reflection to invoke convertRowSchemaToViewSchema.
      Method convertRowSchemaToViewSchemaMethod = SemanticAnalyzer.class.getDeclaredMethod("convertRowSchemaToViewSchema", RowResolver.class);
      convertRowSchemaToViewSchemaMethod.setAccessible(true);
      List<FieldSchema> resSchema = (List<FieldSchema>) convertRowSchemaToViewSchemaMethod.invoke(this, pctx.getOpParseCtx().get(sinkOp).getRowResolver());
      Field resultSchemaField = SemanticAnalyzer.class.getDeclaredField("resultSchema");
      resultSchemaField.setAccessible(true);
      resultSchemaField.set(this, resSchema);
      
      // Run Hive optimization.
      ParseContext pCtx = getParseContext();
      Optimizer optm = new Optimizer();
      optm.setPctx(pCtx);
      optm.initialize(conf);
      pCtx = optm.optimize();
      initParseCtx(pCtx);
      
      // Replace Hive physical plan with Shark plan. This needs to happen after
      // Hive optimization.
      
      if (sinkOp.getNumParent() == 0 && sinkOp.getNumChild() == 0) {
        sinkOp.setParentOperators(op);
      }
      
      genMapRedTasks(pCtx, sinkOp);
      
      LOG.info("Completed plan generation");
    } catch (Exception ex) {
      ex.printStackTrace();
      LOG.error(ex.getMessage(), ex);
    }
  }
  
  @SuppressWarnings("unchecked")
  private void genMapRedTasks(ParseContext pctx, FileSinkOperator sinkOp) throws SemanticException {
    OperatorParser parser = new OperatorParser(pctx, sinkOp);
    IdgsOperator operator = null;
    try {
      operator = parser.parseOperator();
    } catch (IdgsParseException e) {
      throw new SemanticException(e.getMessage(), e);
    }
    
    if (operator == null) {
      throw new SemanticException("parse operator error");
    }
    
    IdgsTask task = (IdgsTask) TaskFactory.get(new IdgsWork(operator), conf);
    rootTasks.add(task);
  }
  
}
